<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="T2IScoreScore (TS2) is a meta-metric for analyzing the consistency of text-to-image faithfulness metrics.">
  <meta name="keywords" content="T2IScoreScore, TS2">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>T2IScoreScore (TS2): Assessing Consistency of T2I Faithfulness Metrics</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro|Share+Tech+Mono"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script
      type="text/javascript"
      src="./static/js/sort-table.js"
      defer
    ></script>

<!-- logo stuff by Michael -->
<style>
.logostyle {
    font-weight: bold;
    background: -webkit-linear-gradient(#444393, #09A5D9); 
    -webkit-background-clip: text; 
    -webkit-text-fill-color: transparent;
    /*
    font-family: 'Share Tech Mono', monospace;
    font-size: 1.05em;
    */
}
#results {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 16px;
  border-collapse: collapse;
  width: 100%;
  text-align: center;
  margin: auto;
  margin-top: 1em;
  margin-bottom: 2em;
}
thead {
  background: #ddd;
}

#results table th {
  border: 1px solid #ddd;
  padding: 8px;
  padding-top: 12px;
  padding-bottom: 12px;
  font-size: 16px;
  text-align: center;
  font-weight: 600;
  background-color: var(--UCLA-darkblue);
  color: white;
}

table#results.js-sort-0 tbody tr td:nth-child(1),
table#results.js-sort-1 tbody tr td:nth-child(2),
table#results.js-sort-2 tbody tr td:nth-child(3),
table#results.js-sort-3 tbody tr td:nth-child(4),
table#results.js-sort-4 tbody tr td:nth-child(5),
table#results.js-sort-5 tbody tr td:nth-child(6),
table#results.js-sort-6 tbody tr td:nth-child(7),
table#results.js-sort-7 tbody tr td:nth-child(8),
table#results.js-sort-8 tbody tr td:nth-child(9),
table#results.js-sort-9 tbody tr td:nth-child(10),
table#results.js-sort-10 tbody tr td:nth-child(11),
table#results.js-sort-11 tbody tr td:nth-child(12),
table#results.js-sort-12 tbody tr td:nth-child(13),
table#results.js-sort-13 tbody tr td:nth-child(14),
table#results.js-sort-14 tbody tr td:nth-child(15) {
  border-left: #dee;
}

#results td {
  border: 1px solid #ddd;
  padding: 8px;
}


#results tr:hover {
  background-color: #ddd;
}

.js-sort-number:hover {
  text-decoration: underline;
  cursor: pointer;
}

.best-score-text {
  color: #c6011f;
}

.tifa {
  background-color: rgb(252, 220, 220);
}

.dsg {
  background-color: rgb(220, 239, 245);
}

.llmscore {
  background-color: lightgoldenrodyellow;
}

@font-face {
    font-family: "PlayStation";
    src: url("./static/fonts/EmotionEngineBold.ttf");
  }

  @font-face {
    font-family: "PSLogo";
    src: url("./static/fonts/Perspire.ttf");
  }

  .psfont {
    font-family: "PlayStation";
    font-variant:small-caps; 
  }
  
  .pslogo {
    font-family: "PSLogo";
  }

  .list {
    margin-left: 1em;
  }

  .list li {
    list-style-type: circle;
    margin-top: 0.2em;
    margin-bottom: 0.2em;
  }
  
  p a {
    text-decoration: none;
    font-weight:600;
    color: rgb(50, 115, 228) !important;
  }

  li a {
    text-decoration: none;
    font-weight:600;
    color: rgb(50, 115, 228) !important;
  }

  p a:hover {
    text-decoration: underline;
  }

  li a:hover {
    text-decoration: underline;
  }


</style>

</head>
<body>

<!--nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> 
            <!--img src="./static/images/ts2logo.svg" style="height: 0.7em;" alt="TS2"/--> 
            <emph class="logostyle psfont">T2IScoreScore</emph> <emph class="logostyle">(<img src="./static/images/ts2logo.svg" style="height: 0.6em;" alt="TS2"/>):</emph> Objectively assessing text-to-image prompt faithfulness metrics 
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://saxon.me">Michael Saxon</a><sup><img width="15pt" src="static/images/psx.svg"/><img width="15pt" src="static/images/psc.svg"/></sup>
            </span>
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://arenaa.github.io/">Mahsa Khoshnoodi</a><sup><img width="15pt" src="static/images/psx.svg"/><img width="15pt" src="static/images/pst.svg"/></sup></span>
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://fatimajahara.com/">Fatima Jahara</a><sup><img width="15pt" src="static/images/psx.svg"/><img width="15pt" src="static/images/pst.svg"/></sup></span>
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://yujielu10.github.io/">Yujie Lu</a><sup><img width="15pt" src="static/images/psc.svg"/></sup>
            </span>
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://scholar.google.com/citations?user=4SRc4qkAAAAJ">Aditya Sharma</a><sup><img width="15pt" src="static/images/psc.svg"/></sup>
            </span>
            <span class="author-block" style="padding-right:6pt;">
              <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a><sup><img width="15pt" src="static/images/psc.svg"/></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-style: italic;padding-right:6pt;"><sup><img width="15pt" src="static/images/psx.svg"/></sup><b>Co-first authors</b></span>
            <span class="author-block" style="padding-right:6pt;"><sup><img width="15pt" src="static/images/psc.svg"/></sup>University of California, Santa Barbara</span>
            <span class="author-block" style="padding-right:6pt;"><sup><img width="15pt" src="static/images/pst.svg"/></sup>Fatima Al-Fihri Predoctoral Fellowship</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.04251"
                   class="external-link button is-normal is-rounded is-dark"
                   style="background-color: #c6011f;">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/michaelsaxon/T2IScoreScore"
                   class="external-link button is-normal is-rounded is-dark"
                   style="background: -webkit-linear-gradient(#444393, #09A5D9);">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/saxon/T2IScoreScore"
                   class="external-link button is-normal is-rounded is-light"
                   style="background-color: #FFD21E;">
                  <span class="icon">
                      <img src="./static/images/hf-logo.svg" style="height:2em;"/>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop has-text-centered content">
          <img src="static/images/ts2teaser.svg" style="width: 100%;"
          alt="The teaser figure for TS2."/>
  </div>
  </div>
  
</section>


<section class="hero teaser">

  <div class="container is-max-desktop">


    <div class="body">
      <h2 class="subtitle ">
        <emph class="logostyle psfont">T2IScoreScore</emph> (<emph class="logostyle">TS2</emph>) assesses how well a T2I metric can correctly <b>order</b> 
        and <b>separate</b> clusters of images with different error counts with respect to a single prompt using a <b>semantic error graph</b>.
      </h2>
      <h3 class="subtitle ">
        Surprisingly, we find that the <b>state-of-the-art LM-based metrics</b> (in terms of human preference correlations) <b>struggle to outperform simple feature-space
        metrics like CLIPScore</b> at ordering or separating closely related sets of images. This surprising result is contrary to claims in many papers presenting novel metrics;
        the benefits of using more complicated systems become much less pronounced in more challenging and realistic evaluation settings such as ours.
      </h3>
      <h3 class="subtitle ">
        Our benchmark dataset consists of 165 prompts, each with a connected <emph>Semantic Error Graph</emph> (SEG) of 4 to 76 images arranged by specific semantic errors relative to the prompt (see above image for example).
        Evaluating text-to-image prompt faithfulness metrics in this way is more rigorous than human preference evaluations which don't consider fine-grained, near neighbor images against the same prompt (which is what T2I metrics are supposed to measure).
        <emph class="logostyle">TS2</emph> can evaluate any T2I faithfulness metric as a <b>black-box</b>.
      </h3>
      &nbsp;
    </div>
  </div>
</section>





<section class="hero is-light ">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h3 class="title is-4">Check out our <emph class="logostyle">Interactive Leaderboard!</emph></h3>
      <p>
        <b>Click on any column title to sort the table by that variable.</b> Deeper method details follow table. Explanation of columns: 
      </p>

      <ul class="list">
        <li><b>Ord</b> is a measure of how well a metric correctly orders images by semantic error count based on <a href="https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient">Spearman's correalation</a>.</li>
        <li><b>Sep</b> is a measure of how well a metric separates pairs of clusters that differ in semantic errors based on the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test">two-sample Kolmogorov&ndash;Smirnov statistic</a>.</li>
        <li><b>*<sub>Avg</sub></b> is average of measure <b>*</b> over all Semantic Error Graphs (SEGs) in <emph class="logostyle">TS2</emph>.</li>
        <li><b>*<sub>Synth</sub></b> is the average of measure <b>*</b> over our artificially designed SEGs. <b>These are the easiest types of errors to detect</b>.</li>
        <li><b>*<sub>Nat</sub></b> is the average of measure <b>*</b> over SEGs using <b>real stock photos</b> rather than generated images, with artificially constructed "prompts" to place them in an error graph.</li>
        <li><b>*<sub>Real</sub></b> is the average over SEGs containing <i>natural errors</i> from real text-to-image model generation attempts. <b>This is both the hardest to score and the most construct-valid</b>.</li>
        <li>Because we tested several different backend LLMs for <span class="dsg">DSG</span> and <span class="tifa">TIFA</span>, their rows are color coded for readability.</li>
      </ul>
    </div>
    <table class="js-sort-table" id="results">
      <thead>
        <tr >
          <td></td>
          <td colspan="2"><strong>Average (All Images)</strong></td>
          <td colspan="2"><strong>Synthetic Errors</strong></td>
          <td colspan="2"><strong>Natural Images</strong></td>
          <td colspan="2"><strong>Natural Errors</strong></td>
        </tr>
        <tr style="background-color: rgb(213,213,213);">
          <td class="js-sort-number"><strong>Method</strong></td>
          <td class="js-sort-number"><strong>Ord<sub>Avg</sub></strong></td>
          <td class="js-sort-number"><strong>Sep<sub>Avg</sub></strong></td>
          <td class="js-sort-number"><strong>Ord<sub>Synth</sub></strong></td>
          <td class="js-sort-number"><strong>Sep<sub>Synth</sub></strong></td>
          <td class="js-sort-number"><strong>Ord<sub>Nat</sub></strong></td>
          <td class="js-sort-number"><strong>Sep<sub>Nat</sub></strong></td>
          <td class="js-sort-number"><strong>Ord<sub>Real</sub></strong></td>
          <td class="js-sort-number"><strong>Sep<sub>Real</sub></strong></td>  
        </tr>
      </thead>
      <tr class="dsg">
        <td>DSG + InstructBLIP</td>
        <td><b class="best-score-text">0.802</b></td>
        <td>0.843</td>
        <td><b class="best-score-text">0.861</b></td>
        <td>0.888</td>
        <td>0.702</td>
        <td>0.815</td>
        <td>0.658</td>
        <td>0.689</td>
      </tr>
      <tr class="dsg">
        <td>DSG + LLaVA-1.5</td>
        <td><u><b>0.800</b></u></td>
        <td>0.825</td>
        <td><u><b>0.838</b></u></td>
        <td>0.855</td>
        <td><b class="best-score-text">0.749</b></td>
        <td>0.751</td>
        <td><u><b>0.696</b></u></td>
        <td>0.768</td>
      </tr>
      <tr class="dsg">
        <td>DSG + BLIP1</td>
        <td>0.769</td>
        <td>0.806</td>
        <td>0.817</td>
        <td>0.841</td>
        <td><u><b>0.710</b></u></td>
        <td>0.751</td>
        <td>0.628</td>
        <td>0.714</td>
      </tr>
      <tr class="tifa">
        <td>TIFA + InstructBLIP</td>
        <td>0.765</td>
        <td>0.850</td>
        <td>0.802</td>
        <td>0.867</td>
        <td>0.651</td>
        <td>0.828</td>
        <td><b class="best-score-text">0.716</b></td>
        <td>0.805</td>
      </tr>
      <tr class="dsg">
        <td>DSG + LLaVA-1.5 (w/prompt eng)</td>
        <td>0.756</td>
        <td>0.805</td>
        <td>0.821</td>
        <td>0.838</td>
        <td>0.689</td>
        <td>0.772</td>
        <td>0.559</td>
        <td>0.706</td>
      </tr>
      <tr class="tifa">
        <td>TIFA + LLaVA-1.5</td>
        <td>0.745</td>
        <td>0.843</td>
        <td>0.792</td>
        <td>0.875</td>
        <td>0.628</td>
        <td>0.834</td>
        <td>0.667</td>
        <td>0.727</td>
      </tr>
      <tr class="tifa">
        <td>TIFA + LLaVA-1.5 (w/prompt eng)</td>
        <td>0.744</td>
        <td>0.819</td>
        <td>0.792</td>
        <td>0.852</td>
        <td>0.640</td>
        <td>0.756</td>
        <td>0.645</td>
        <td>0.744</td>
      </tr>
      <tr>
        <td>ALIGNScore</td>
        <td>0.739</td>
        <td><b class="best-score-text">0.928</b></td>
        <td>0.776</td>
        <td><b class="best-score-text">0.941</b></td>
        <td>0.702</td>
        <td><b class="best-score-text">0.926</b></td>
        <td>0.626</td>
        <td><u><b>0.879</b></u></td>
      </tr>
      <tr class="tifa">
        <td>TIFA+ BLIP1</td>
        <td>0.738</td>
        <td>0.818</td>
        <td>0.788</td>
        <td>0.841</td>
        <td>0.622</td>
        <td>0.779</td>
        <td>0.640</td>
        <td>0.764</td>
      </tr>
      <tr>
        <td>CLIPScore</td>
        <td>0.714</td>
        <td><u><b>0.907</b></u></td>
        <td>0.750</td>
        <td><u><b>0.905</b></u></td>
        <td>0.580</td>
        <td><u><b>0.915</b></u></td>
        <td>0.693</td>
        <td><b class="best-score-text">0.903</b></td>
      </tr>
      <tr class="tifa">
        <td>TIFA + MPlug</td>
        <td>0.710</td>
        <td>0.806</td>
        <td>0.726</td>
        <td>0.806</td>
        <td>0.669</td>
        <td>0.842</td>
        <td>0.682</td>
        <td>0.774</td>
      </tr>
      <tr class="dsg">
        <td>DSG + MPlug</td>
        <td>0.688</td>
        <td>0.755</td>
        <td>0.735</td>
        <td>0.771</td>
        <td>0.619</td>
        <td>0.706</td>
        <td>0.564</td>
        <td>0.731</td>
      </tr>
      <tr>
        <td>LLMScore Over</td>
        <td>0.577</td>
        <td>0.735</td>
        <td>0.616</td>
        <td>0.728</td>
        <td>0.444</td>
        <td>0.767</td>
        <td>0.541</td>
        <td>0.736</td>
      </tr>
      <tr>
        <td>LLMScore EC</td>
        <td>0.488</td>
        <td>0.736</td>
        <td>0.502</td>
        <td>0.711</td>
        <td>0.362</td>
        <td>0.805</td>
        <td>0.544</td>
        <td>0.773</td>
      </tr>
      <tr class="tifa">
        <td>TIFA + Fuyu</td>
        <td>0.387</td>
        <td>0.672</td>
        <td>0.445</td>
        <td>0.673</td>
        <td>0.235</td>
        <td>0.757</td>
        <td>0.297</td>
        <td>0.593</td>
      </tr>
      <tr>
        <td>VIEScore + LLaVA-1.5</td>
        <td>0.378</td>
        <td>0.518</td>
        <td>0.425</td>
        <td>0.537</td>
        <td>0.224</td>
        <td>0.445</td>
        <td>0.332</td>
        <td>0.507</td>
      </tr>
      <tr class="dsg">
        <td>DSG + Fuyu</td>
        <td>0.358</td>
        <td>0.660</td>
        <td>0.455</td>
        <td>0.687</td>
        <td>0.215</td>
        <td>0.710</td>
        <td>0.100</td>
        <td>0.508</td>
      </tr>
    </table>
  <div class="container is-max-desktop">
    <p>
      We evaluate the CLIPScore, ALIGNScore, TIFA, DSG, LLMScore, and VIEScore metrics, using a variety of VQA and VLM models where appropriate.
      On all separation scores, the simplest CLIP and ALIGNScore methods outperform the more complex LLM-based metrics.
      On the hardest partition for the ordering score, <b>Ord<sub>Real</sub></b>, CLIPScore comes within 2% the performance of the best metric.
    </p>
  </div>
</div>
</section>


<!--section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their performance in terms <i>prompt faithfulness</i>&mdash;semantic coherence of generated images to their conditioning prompts.
            Leveraging advances in cross-modal embeddings and vision-language models (VLMs), a variety of T2I faithfulness metrics have been proposed.
            However, these metrics are not rigorously compared and benchmarked, instead presented against a single weak baseline using correlation to human Likert scores over a set of easy-to-discriminate images. 
            We introduce <emph class="logostyle">T2IScoreScore</emph>, a carefully curated set of 
            <i>semantic error graphs</i> consisting of a prompt and a set of images containing increasing numbers of errors. 
            This allows us to rigorously judge whether a prompt faithfulness metric under test can correctly order images with respect to their objective error count and significantly discriminate 
            between different error nodes novel scores derived from established statistical tests.
            Surprisingly, we find that the state-of-the-art VLM-based 
            metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model 
            errors.
            <emph class="logostyle">T2IScoreScore</emph> will enable the development of better T2I prompt faithfulness metrics by enabling their rigorous comparison on near-neighbor images with significant semantic differences.

          </p>
        </div>
      </div>
    </div>
  </div>
</section-->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Details on the <emph class="logostyle">TS2</emph> Semantic Error Graphs (SEGs)</h2>
        <div class="content has-text-justified">
          <p>
            Images in <emph class="logostyle">TS2</emph> are collected from a variety of sources, including multiple Stable Diffusion variants, DALL-E 2, and the free stock image website <a href="https://www.pexels.com/">Pexels</a>.
            Prompts that weren't hand-written were sampled from <a href="https://cocodataset.org/#home">MS COCO</a> and <a href="https://huggingface.co/datasets/nateraw/parti-prompts">PartiPrompts</a>.
            The SEGs contain a variety of errors, including missing objects, incorrect object properties, composition errors, and verb errors. SEG distributions depicted:
          </p>
          <img src="static/images/distribution.png" style="width: 100%;" alt="The distribution of errors in our dataset."/>
          <p>
            As mentioned above, <emph class="logostyle">TS2</emph> contains three partitions of SEGs; <b>Synthetic Errors</b>, <b>Natural Images</b>, and <b>Natural Errors</b>.
            They primarily differ in the order in which the images, prompt, and error graph were produced.
            For the <b>Synth</b> group of SEGs, from an initial prompt an error graph was designed, and images were generated to fill the nodes in the graph. (left side of below figure).
            For the <b>Nat</b> group of SEGs, images were collected from stock photo websites and prompts were designed to place them in a constructed error graph (middle of below figure).
            For the <b>Real</b> group of SEGs, a single prompt was used to produce many images, including <b>real T2I generation errors.</b> These were then sorted into an error graph (right of below figure).
          </p>
          &nbsp;
          <img src="static/images/ts2_three_types.drawio.svg" style="width: 100%;" alt="The distribution of errors in our dataset."/>
        </div>
      </div>
    </div>
  </div>
  <br>&nbsp;<br>&nbsp;
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Details on the <emph class="logostyle">TS2</emph> meta-metrics</h2>
        <div class="content has-text-justified">
          <p>
            If you really want to know how the meta-metrics work, you should probably <a href="https://arxiv.org/abs/2404.04251">read the paper!</a>
            But basically, they are assessed by having a metric score each image through every walk down each error graph, 
            using Spearman's rank correlation coefficient against expected error count within each walk to assess correctness of ordering,
            and the two-sample Kolmogorov&ndash;Smirnov statistic is assessed between every adjacent pair of nodes in a walk to assess how well
            a metric separates semantically different populations of images relative to a specific prompt.
          </p>
          <p>
            The dynamic range of both metrics ranges from 0 to 1 (in the paper we scale this to 100 for readability). Although the scores may look high
            already, in principle this should be a very easy task. Although the semantic errors we mark are "objective", this objectivity is implictly 
            a human judgement; thus the human performance barrier on this task is near 100%. <b>For a quality T2I faithfulness metric, <i>this effect</i> is really the most important one to capture.</b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="content has-text-justified">
          <p>
            Our work is fundamentally based on evaluating other groups' metrics. <a href="https://wysiwyr-itm.github.io/">SeeTrue</a> is the closest work to our philosophy,
            with its similar focus on using near-neighbor images to assess T2I models, and investigating mutli-image to single prompt and multi-prompt to single image settings, and also use images.
            However, they focus more using these examples to build a faithfulness metric that can be used to assess VNLI and VQA tasks directly, rather than using them to evaluate T2I metrics themselves.
          </p>
          <p>
            The main metrics we analyzed, <a href="https://tifa-benchmark.github.io/">TIFA</a> and <a href="https://google.github.io/dsg/">DSG</a> from Yushi Hu and Jaemin Cho are both very influential.
            We hope that TS2-guided evaluation will enable better metrics inspired from their work.
          </p>
          <p>
            The captioning based metrics we evaluated, <a href="https://github.com/yujielu10/llmscore">LLMScore</a> and <a href="https://tiger-ai-lab.github.io/VIEScore/">VIEScore</a> both stand
            to benefit a lot from advancements in VLMs and captioning models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>

  <div class="hero container">
    &nbsp;<br>&nbsp;
  </div>
  
  <div class="container is-max-desktop content" id="BibTeX">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{saxon2024evaluates,
      title={Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)}, 
      author={Michael Saxon and Fatima Jahara and Mahsa Khoshnoodi and Yujie Lu and Aditya Sharma and William Yang Wang},
      year={2024},
      eprint={2404.04251},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.04251}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the <a href="https://nerfies.github.io">Nerfies demo page</a>, which is
            licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Use it's <a href="https://github.com/nerfies/nerfies.github.io">source code</a> to reproduce and follow its terms of linking back.
          </p>
          <div class="container has-text-centered">
            <a href="https://www.youtube.com/watch?v=m2OR_JaXDaM"><img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmadjackyl.tripod.com%2Fanimcodec.gif&f=1&nofb=1&ipt=3e15c353fce2f04c8394d8d21cd70f9276d05c4a7ab185640485e921e00b9104&ipo=images" style="height:8em;"/></a>
          </div>
          <p>Not gonna lie, making a website with Bulma CSS with zero prior knowledge of the framework is a massive pain in the ass. But at least it looks better than my <a href="https://saxon.me/coco-crola/">piece of shit</a> <a href="https://asulidarset.github.io/">previous demos</a>!</p>

        </div>
      </div>
    </div>
</footer>

</body>
</html>
